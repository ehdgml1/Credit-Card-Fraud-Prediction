# -*- coding: utf-8 -*-
"""신용카드 사기거래 예측

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mx8s-VBHiRNsSye2lq9tcZneIjjVyvkn
"""

# ▶ Warnings 제거
import warnings
warnings.filterwarnings('ignore')

# ▶ Google drive mount or 폴더 클릭 후 구글드라이브 연결
from google.colab import drive
drive.mount('/content/drive')

# ▶ 경로 설정 (※ Colab을 활성화시켰다면 보통 Colab Notebooks 폴더가 자동 생성)
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/')
# '/content/drive/MyDrive/Colab Notebooks/ ← 여기 경로까지는 본인의 경로 입력, part3_lecture_fastcampus/chapter02' ← 변경X
os.getcwd()

import numpy as np
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/data 모음/credit_fraud.csv')

"""## 가상 문제상황 정의

최근 금융사 A는 비트코인 열풍으로 수수료 거래 금액이 늘어 좋으면서도 한편으로는 골치를 썩고있다.  그 이유는 바로 사기거래 때문이다. 사기거래의 횟수도 증가하며 Claim을 넣는 고객들도 늘어나고 있다 이에 A사는 사기거래로 예측되는 거래건에 경우 거래를 제한하려고 한다.

## 데이터 살펴보기

## 컬럼별 뜻

|account_age_days| transaction_amt| transaction_adj_amt|historic_velocity|ip_address|user_agent|email_domain|phone_number|billing_city|billing_postal|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|계좌 생성후 지난일|거래금액|거래 조정 금액|과거 거래금액|IP주소|사용환경|email 도메인|전화번호|청구도시|청구우편번호
|billing_state|card_bin|currency|cvv|signature_image|transaction_type|transaction_env|EVENT_TIMESTAMP|applicant_name|billing_address|
|청구주|카드bin번호(앞6자리)|통화|CVV|서명이미지|거래종류|거래환경|거래일자|신청자 이름|청구주소|
|merchant_id|locale|tranaction_initiate|days_since_last_logon|inital_amount|EVENT_LABEL|
|상점ID|지역|거래초기코드|마지막로그인후경과일|초기잔액|사기여부|
"""

df.head()

"""# **문제 해결 프로세스 정의**

## **문제 정의**

사기거래로 인한 고객 Claim증가, 브랜드 이미지 감소, 고객 탈퇴

### **기대효과**
사기거래 거래 제한으로 고객 Claim감소, 브랜드 이밎 회복, 고객방어

### **성과 측정**
모델 적용 전과 후를 비교해 사기거래의 Claim감소 건수 확인 가능

### **현업 적용 과정**
1. 카드 Spending data 실시간으로 수집

2. 거래 발생시 마다 모든 거래에 대해 Data모델 Input후 사기거래 의심 건수 추출

3. 사기 거래 가능성이 높은 거래 건수에 대해 제한 조치 실행
"""

df.shape

df.info()

df.isnull().sum()

#결측치의 데이터가 작아서 삭제해도 될듯함
df = df.dropna(axis=0)

df.shape
df.info()

df.describe()

# target에 대한 정보를 보기 편하게 사기거래는 1 아닌 것은 0으로 변경
df['EVENT_LABEL'] = np.where(df['EVENT_LABEL'] == 'fraud' , 1,0)

df['EVENT_LABEL'].value_counts()

"""5%정도의 사기 거래율을 나타냄

"""

df['user_agent'].head(1)

"""lambda식을 활용해 고객 사용환경의 값들중 앞부분의 값들만 따와 보겟습니다."""

df['user_agent'] = df['user_agent'].apply(lambda x : x.split('/')[0])

df.head(3)

"""## 의미있는 변수 선택
- 의미있는 변수란
- 해당변수에 의해서 예측하고자 하는 데이터가 잘 구분 되어지는 변수
## Numeric 변수
연속형 변수는 구간화를 통해 범주형 변수로 변경하고, 잘 구분 되어지는지 확인
## Categorical변수
각 그룹별 예측하고자 하는 데이터가 잘 나눠지는지 확인
"""

# 범주형 변수와 숫자형 변수 나누기
numeric_list=[]
categoical_list=[]

for i in df.columns :
  if df[i].dtypes == 'O' :
    categoical_list.append(i)
  else :
    numeric_list.append(i)

print("categoical_list :", categoical_list)
print("numeric_list :", numeric_list)

#숫자형 변수이름 뽑아보기
print("numeric_list :", numeric_list)

#숫자형 변수만 뽑아보기
df[numeric_list]

# Commented out IPython magic to ensure Python compatibility.
#시각화를 통해 거래금액(transaction_amt)에 대한 분포 확인
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use(['dark_background'])
sns.distplot(df['transaction_amt'])

# 구간화를 통해 유의미한 insight창출
import numpy as np
df['transaction_amt_gp'] = np.where(df['transaction_amt'] <=2000,1,np.where(df['transaction_amt'] <=3000,2,3))

df[['transaction_amt','transaction_amt_gp']]

#구간화에 따른 사기거래
df.groupby(['transaction_amt_gp','EVENT_LABEL'])['transaction_amt'].count()

# 그룹에 따라 사기거래률이 적절하게 나누어 졌다고 볼수 있음
# 3번 그룹의 사기거래율이 높다 보통 가격이 높을수록 사기의 빈도수가 늘어나는듯?!?
print("group1 :", (353 / (353+29420))*100)
print("group2 :", (4825 / (4825+79588))*100)
print("group3 :", (2826 / (2826+29988))*100)

#dataframe형태로 만들기
df_tran = pd.DataFrame(df.groupby(['transaction_amt_gp', 'EVENT_LABEL'])['EVENT_TIMESTAMP'].count()).reset_index()
df_tran.head(5)

df_pivot = pd.pivot_table(df_tran,
                          index = 'transaction_amt_gp',
                          columns = 'EVENT_LABEL',
                          values = 'EVENT_TIMESTAMP')

df_pivot = df_pivot.reset_index()
df_pivot.columns.names = ['']
df_pivot.head(5)

#ratio비율 추가해주기
df_pivot['ratio'] = round((df_pivot.iloc[:,2] /(df_pivot.iloc[:,1] + df_pivot.iloc[:,2]) * 100 ),2)
df_pivot

categoical_list

df['EVENT_TIMESTAMP'].count()

df[categoical_list].head()

#user agent(인터넷 블라우저 사용환경)에 따른 사기거래율 비교하기
# 두가지의 사용 환경이고 비율은 비슷한거 같음
sns.catplot(x='user_agent', hue='EVENT_LABEL', kind='count', palette='pastel',edgecolor='.6',data=df);
plt.gcf().set_size_inches(10,3)

cat_val = 'user_agent'

df_tran = pd.DataFrame(df.groupby([cat_val , 'EVENT_LABEL'])['EVENT_TIMESTAMP'].count()).reset_index()#총 횟수를 셀수있는 카테고리를 이용한거 같음
df_tran.head()

df_pivot = pd.pivot_table(df_tran,
                        index=cat_val,
                        columns = 'EVENT_LABEL',
                        values = 'EVENT_TIMESTAMP')

df_pivot = df_pivot.reset_index()
df_pivot.columns.names = ['']

df_pivot['ratio'] = round((df_pivot.iloc[:,2]) / (df_pivot.iloc[:,1] + df_pivot.iloc[:,2])*100,1)
df_pivot

df['currency'].value_counts()

#거래통화에 따른 사기거래율 비교
sns.catplot(x='currency', hue= 'EVENT_LABEL',kind='count',palette='pastel',edgecolor='.6',data=df);
plt.gcf().set_size_inches(10,3)

cat_val='currency'

# ▶ 1단계
df_tran = pd.DataFrame(df.groupby([cat_val, 'EVENT_LABEL'])['EVENT_TIMESTAMP'].count()).reset_index()
df_tran.head(5)

# ▶ 2단계
df_pivot = pd.pivot_table(df_tran,                   # 피벗할 데이터프레임
                      index = cat_val,               # 행 위치에 들어갈 열
                      columns = 'EVENT_LABEL',       # 열 위치에 들어갈 열
                      values = 'EVENT_TIMESTAMP')    # 데이터로 사용할 열 
# ▶ 3단계
df_pivot = df_pivot.reset_index()
df_pivot.columns.names=['']

# ▶ 4단계
df_pivot['ratio'] =   round((df_pivot.iloc[:,2])/ (df_pivot.iloc[:,1]+df_pivot.iloc[:,2]) * 100,1)
df_pivot

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

X = df.drop(['EVENT_TIMESTAMP', 'EVENT_LABEL','transaction_amt_gp'],axis=1)
Y = df['EVENT_LABEL']
x_train, x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,stratify=Y)

categoical_list.remove('EVENT_TIMESTAMP')

print(categoical_list)

from sklearn.preprocessing import LabelEncoder

for col in categoical_list:
  print(col)
  le=LabelEncoder()
  le.fit(list(x_train[col].values) + list(x_test[col].values))
  x_train[col] = le.transform(x_train[col])
  x_test[col] = le.transform(x_test[col])

x_train[categoical_list].head(3)

x_test[categoical_list].head(3)

from sklearn.metrics import classification_report
rfc= RandomForestClassifier(random_state=123456,max_depth=8,n_estimators=500)
rfc.fit(x_train,y_train)

#예측
y_pred_train=rfc.predict(x_train)
y_pred_test=rfc.predict(x_test)

print(classification_report(y_train,y_pred_train))
print(classification_report(y_test,y_pred_test))

pd.Series(y_pred_test).value_counts()

#ROC
#이진 분류기의 성능을 평가할때 사용하는 지표(metric),100에 가까울수록 모델 성능이 좋은것(다양한 treshold에 대한 이진 분류기의 성능을 한번에 표시한것)
#과적합 문제 Train과 test set에 성능을 최대한 줄여주는 것이 과적합을 방지
from sklearn.metrics import roc_auc_score

y_pred_train_proba = rfc.predict_proba(x_train)[:,1]
y_pred_test_proba = rfc.predict_proba(x_test)[:,1]

roc_score_train = roc_auc_score(y_train,y_pred_train_proba)
roc_score_test = roc_auc_score(y_test,y_pred_test_proba)

print("roc_score_train : {}".format(roc_score_train))
print("roc_score_test : {}".format(roc_score_test))

"""# 중요변수 파악"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use(['dark_background'])

ftr_importance_values = rfc.feature_importances_
ftr_importances = pd.Series(ftr_importance_values,index=x_train.columns)
ftr_top20 = ftr_importances.sort_values(ascending=False)[:30]

plt.figure(figsize = (8,6))
plt.title('Feature Importances')
sns.barplot(x=ftr_top20,y=ftr_top20.index)

plt.show()

"""확실히 금액에 따라 사기거래를 판별하는 것이 가장 핵심인것 같다"""

# 1번 중요 변수에 대한 상세 탐색
sns.distplot(df['transaction_adj_amt']);

# ▶ 구간화
import numpy as np
df['transaction_adj_amt_group'] = np.where (df['transaction_adj_amt'] <= 40, 1, 
                           np.where(df['transaction_adj_amt'] <= 60, 2, 3))

df[['transaction_adj_amt','transaction_adj_amt_group']]

df.groupby(['transaction_adj_amt_gp', 'EVENT_LABEL'])['EVENT_LABEL'].count()

# 조정된 금액이 작을수록 사기거래 확률이 급격시 상승한다. ( Critical한 변수임을 알수 있다.)
print("group1 :", (4628 / (4628+8072)) * 100)
print("group2 :", (3131 / (3131+92394)) * 100)
print("group3 :", (245 / (245+38530)) * 100)

# 2번 중요 변수에 대한 상세 탐색
sns.distplot(df['account_age_days'])

# ▶ 구간화
import numpy as np
df['account_age_days_group'] = np.where (df['account_age_days'] <= 3500, 1, 
                           np.where(df['account_age_days'] <= 4500, 2, 3))

df[['account_age_days','account_age_days_group']]

df.groupby(['account_age_days_graph', 'EVENT_LABEL'])['EVENT_TIMESTAMP'].count()

# 생성일이 오래 된 계좌일수록 사기거래률이 증가 눈에 확실하게 보임
print("group1 :", (190 / (24908+190)) * 100)
print("group2 :", (1087 / (1087+39636)) * 100)
print("group3 :", (6727 / (6727+74452)) * 100)

"""생성일이 좀 지난 계좌가 확실히 사기거래에 많이 이용되는거 같다

## 모델 Save and Read
"""

import pickle
# 모델 저장
saved_model = pickle.dumps(rfc)

# 모델 Read
clf_from_pickle = pickle.loads(saved_model)

